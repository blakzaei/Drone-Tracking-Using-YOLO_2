{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8242216,"sourceType":"datasetVersion","datasetId":4889407},{"sourceId":8250339,"sourceType":"datasetVersion","datasetId":4895225},{"sourceId":8261288,"sourceType":"datasetVersion","datasetId":4903407},{"sourceId":8262666,"sourceType":"datasetVersion","datasetId":4904421},{"sourceId":8263234,"sourceType":"datasetVersion","datasetId":4904853},{"sourceId":8532908,"sourceType":"datasetVersion","datasetId":5096472},{"sourceId":8758731,"sourceType":"datasetVersion","datasetId":5262134},{"sourceId":8946358,"sourceType":"datasetVersion","datasetId":5383587},{"sourceId":206001255,"sourceType":"kernelVersion"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#-- Matching Same Object --","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython import display","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-22T14:36:01.853055Z","iopub.execute_input":"2024-11-22T14:36:01.853409Z","iopub.status.idle":"2024-11-22T14:36:01.868639Z","shell.execute_reply.started":"2024-11-22T14:36:01.853368Z","shell.execute_reply":"2024-11-22T14:36:01.867889Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"#-- Install ultralytics ------------------------------------------------------------------------------------------\n!pip install ultralytics\n\ndisplay.clear_output()\n\nimport ultralytics\nultralytics.checks()\n#-----------------------------------------------------------------------------------------------------------------","metadata":{"execution":{"iopub.status.busy":"2024-11-22T14:36:03.932707Z","iopub.execute_input":"2024-11-22T14:36:03.933312Z","iopub.status.idle":"2024-11-22T14:36:17.556257Z","shell.execute_reply.started":"2024-11-22T14:36:03.933275Z","shell.execute_reply":"2024-11-22T14:36:17.555401Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Ultralytics 8.3.36 ðŸš€ Python-3.10.14 torch-2.4.0 CUDA:0 (Tesla T4, 15095MiB)\nSetup complete âœ… (4 CPUs, 31.4 GB RAM, 5933.9/8062.4 GB disk)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"#-- Imports ----------------------------------------------------------------------------------------------------\nfrom ultralytics import YOLO\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.applications.resnet50 import preprocess_input\nfrom tensorflow.keras.models import Model\nfrom collections import defaultdict\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport os\nfrom datetime import datetime, timedelta\nimport shutil\n#-----------------------------------------------------------------------------------------------------------------","metadata":{"execution":{"iopub.status.busy":"2024-11-22T14:36:20.080809Z","iopub.execute_input":"2024-11-22T14:36:20.081304Z","iopub.status.idle":"2024-11-22T14:36:31.279238Z","shell.execute_reply.started":"2024-11-22T14:36:20.081257Z","shell.execute_reply":"2024-11-22T14:36:31.278571Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"#-- Initialize --------------------------------------------------------------------------------------------------\nout_dir = '/kaggle/working/'\ndetection_weights_file = '/kaggle/input/yolo11-11frozen-13/model_11_frozen_epoch_60/train/weights/best.pt'\n\ndrone_files = ['/kaggle/input/drone-dataset-p1/v_5.mp4',\n              '/kaggle/input/drone-dataset-p2/v_8.mp4',\n              '/kaggle/input/drone-detection-test-videos-1/drone_video (1).mp4',\n              # '/kaggle/input/novin-data/Novin_Dataset/f2.part2.mp4',\n              '/kaggle/input/sample-videos-detecting-and-matching-objs-1/sample_video_drone (5).mp4',\n              '/kaggle/input/video-drone-bird-1/Untitled-13.mp4']\n\nresults_dir = out_dir + 'results/'\nif not os.path.exists(results_dir):\n    os.makedirs(results_dir)\n\nAREA_THRESHOLD = 5\nDISTANCE_THRESHOLD = 50\nSIMILARITY_THRESHOLD = 0.6\nCROP_PADDING = 10\nTIME_THRESHOLD = 60\nNUM_TRACK_THRESHOLD = 30\n#-----------------------------------------------------------------------------------------------------------------","metadata":{"execution":{"iopub.status.busy":"2024-11-22T14:36:35.173277Z","iopub.execute_input":"2024-11-22T14:36:35.173653Z","iopub.status.idle":"2024-11-22T14:36:35.179214Z","shell.execute_reply.started":"2024-11-22T14:36:35.173624Z","shell.execute_reply":"2024-11-22T14:36:35.178364Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"#-- Set Detection Model ------------------------------------------------------------------------------------------\nmodel = YOLO(detection_weights_file)  \n#-----------------------------------------------------------------------------------------------------------------","metadata":{"execution":{"iopub.status.busy":"2024-11-22T14:36:37.552862Z","iopub.execute_input":"2024-11-22T14:36:37.553549Z","iopub.status.idle":"2024-11-22T14:36:39.947709Z","shell.execute_reply.started":"2024-11-22T14:36:37.553511Z","shell.execute_reply":"2024-11-22T14:36:39.946983Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"#-- Set Similarity Measure Model ---------------------------------------------------------------------------------\nsimilarity_base_model = ResNet50(weights='imagenet')\n\n#-- Use the second-last layer for embeddings --\nsimilarity_model = Model(inputs=similarity_base_model.input,\n                         outputs=similarity_base_model.layers[-2].output)  \n#-----------------------------------------------------------------------------------------------------------------","metadata":{"execution":{"iopub.status.busy":"2024-11-20T07:38:54.933596Z","iopub.execute_input":"2024-11-20T07:38:54.934265Z","iopub.status.idle":"2024-11-20T07:38:57.745564Z","shell.execute_reply.started":"2024-11-20T07:38:54.934204Z","shell.execute_reply":"2024-11-20T07:38:57.744854Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#-- Function to Preprocess Image for Similarity Measure ---------------------------------------------------------\ndef preprocess_image(image, target_size=(224, 224)):\n    \n    image = cv2.resize(image, target_size) \n    image = np.expand_dims(image, axis=0)  #-- Add batch dimension\n    image = preprocess_input(image)  #-- Normalize for ResNet\n    return image\n#-----------------------------------------------------------------------------------------------------------------","metadata":{"execution":{"iopub.status.busy":"2024-11-20T07:40:42.794005Z","iopub.execute_input":"2024-11-20T07:40:42.794419Z","iopub.status.idle":"2024-11-20T07:40:42.799475Z","shell.execute_reply.started":"2024-11-20T07:40:42.794380Z","shell.execute_reply":"2024-11-20T07:40:42.798582Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#-- Function to Calculate Similarity -----------------------------------------------------------------------------\ndef compare_similarity_images(image1, image2):   \n    \n    #-- Preprocess images --\n    img1 = preprocess_image(image1)\n    img2 = preprocess_image(image2)\n    \n    #-- Extract features --\n    embedding1 = similarity_model.predict(img1)\n    embedding2 = similarity_model.predict(img2)\n\n    #-- Compute cosine similarity --\n    similarity_score = cosine_similarity(embedding1, embedding2)[0][0]     \n\n    return similarity_score\n#-----------------------------------------------------------------------------------------------------------------","metadata":{"execution":{"iopub.status.busy":"2024-11-20T10:04:03.790058Z","iopub.execute_input":"2024-11-20T10:04:03.790859Z","iopub.status.idle":"2024-11-20T10:04:03.795648Z","shell.execute_reply.started":"2024-11-20T10:04:03.790825Z","shell.execute_reply":"2024-11-20T10:04:03.794805Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#-- Function to Match Detected Objects ---------------------------------------------------------------------------\ndef match_object(track_id, track_box, track_image, track_time, last_tracked_objects):   \n    \n    plt.imshow(track_image)\n    plt.title(f'track_object - id:{track_id}')\n    plt.axis('off')  \n    plt.show()    \n    \n    track_center_x, track_center_y, track_w, track_h = track_box     \n    \n    distance_match = {}\n    similarity_match = {}\n    \n    matched_id = None\n    \n    for obj_id, (obj_box, obj_img, obj_time) in last_tracked_objects.items():\n        \n        print(f'####################### {obj_id} #####################')\n        plt.imshow(obj_img)\n        plt.title(f'object- id:{obj_id}')\n        plt.axis('off')  \n        plt.show()    \n        \n        time_difference = abs(track_time - obj_time)\n        print(f'-------- time_difference: {time_difference} -------------')\n        if time_difference > timedelta(seconds=TIME_THRESHOLD):            \n            continue\n        \n        similarity_score = compare_similarity_images(track_image, obj_img)\n        print(f'-------- similarity_score: {similarity_score} -------------')\n        if similarity_score < SIMILARITY_THRESHOLD:            \n            continue\n        \n        x_center, y_center, w, h = obj_box     \n        \n        a_track = track_w * track_h\n        a_obj = w *h        \n        if a_track>a_obj:\n            a_ratio = a_track/a_obj\n        else:\n            a_ratio = a_obj/a_track\n        \n        print(f'-------- a_ratio: {a_ratio} -------------')\n        if a_ratio > AREA_THRESHOLD:            \n            continue         \n        \n        distance = np.sqrt((track_center_x - x_center)**2 + (track_center_y - y_center)**2)      \n        print(f'-------- distance: {distance} -------------')\n        if distance <= DISTANCE_THRESHOLD:            \n            distance_match[obj_id] = distance\n        else:\n            similarity_match[obj_id] = similarity_score\n    \n    if len(distance_match)!=0:\n        matched_id = min(distance_match, key=distance_match.get) \n    elif len(similarity_match)!=0:\n        matched_id = min(similarity_match, key=similarity_match.get)       \n    \n    print(f'-------- matched_id: {matched_id} -------------')\n    return matched_id\n\n      \n#-----------------------------------------------------------------------------------------------------------------","metadata":{"execution":{"iopub.status.busy":"2024-11-20T10:12:46.999146Z","iopub.execute_input":"2024-11-20T10:12:46.999975Z","iopub.status.idle":"2024-11-20T10:12:47.008555Z","shell.execute_reply.started":"2024-11-20T10:12:46.999940Z","shell.execute_reply":"2024-11-20T10:12:47.007679Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def crop_object(frame, box, padding=CROP_PADDING):\n    \n    frame_height, frame_width = frame.shape[:2]    \n    center_x, center_y, w, h = box  \n    \n    top_left_x = int(max(center_x - w // 2 - padding, 0))\n    top_left_y = int(max(center_y - h // 2 - padding, 0))\n    bottom_right_x = int(min(center_x + w // 2 + padding, frame_width))\n    bottom_right_y = int(min(center_y + h // 2 + padding, frame_height))\n    \n    cropped_object = frame[top_left_y:bottom_right_y, top_left_x:bottom_right_x].copy()\n    \n    return cropped_object\n","metadata":{"execution":{"iopub.status.busy":"2024-11-20T09:56:49.477773Z","iopub.execute_input":"2024-11-20T09:56:49.478119Z","iopub.status.idle":"2024-11-20T09:56:49.483723Z","shell.execute_reply.started":"2024-11-20T09:56:49.478089Z","shell.execute_reply":"2024-11-20T09:56:49.482750Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#-- Run ----------------------------------------------------------------------------------------------------------\nfor video_file in drone_files:\n    \n    #-- get video name --\n    index = video_file.rfind('/')      \n    video_name = video_file[index + 1:] \n    \n    #-- set output file --\n    out_video_name = 'out_' + video_name    \n    output_path = results_dir + out_video_name\n    \n    print(f'=== Processing {video_name} ================================')\n    \n    cap = cv2.VideoCapture(video_file)\n    \n    #-- get video properties --\n    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n\n    #-- set video writer --\n    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")  \n    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n    \n    track_history = defaultdict(lambda: []) #-- for tracking\n    last_tracked_objects = {} #-- for matching  \n    mapped_objects = {}\n    frame_number = 0\n    \n    while cap.isOpened():    \n        success, frame = cap.read()\n        if success:            \n            frame_number += 1\n            print(f'\\nframe number = {frame_number} ==================================') \n            print('track_history:', track_history.keys())\n            print('last_tracked_objects:', last_tracked_objects.keys())\n#             if frame_number>=900:\n#                 break            \n            \n            #-- detect and track objects --\n            results = model.track(frame,    \n                                  tracker = 'bytetrack.yaml',\n                                  persist=True,\n                                  show = False)\n\n            #-- Check if there are any detections --\n            if results[0].boxes is not None and results[0].boxes.xywh is not None:            \n                boxes = results[0].boxes.xywh.cpu()\n                track_ids = results[0].boxes.id\n                if track_ids is not None:\n                    track_ids = track_ids.int().cpu().tolist()       \n                                \n                    for box, track_id in zip(boxes, track_ids):     \n\n                        #-- Crop the object from the frame --\n                        cropped_object = crop_object(frame, box)\n                        detection_time = datetime.now()\n                        \n                        #-- Check if this is the first detected object --\n                        if len(last_tracked_objects)==0:\n                            last_tracked_objects[track_id] = (box, cropped_object, detection_time)\n                        else:\n                            #-- Check if its not new detected object --\n                            if track_id in last_tracked_objects:\n                                last_tracked_objects[track_id] = (box, cropped_object, detection_time)                   \n                                \n                            else:   \n                                if track_id in mapped_objects:\n                                    matched_id =  mapped_objects[track_id]\n                                else:\n                                    matched_id = match_object(track_id, box, cropped_object, detection_time, last_tracked_objects)\n                                if matched_id is not None:\n                                    mapped_objects[track_id] = matched_id\n                                    track_id = matched_id                                    \n                                last_tracked_objects[track_id] = (box , cropped_object, detection_time)                      \n                                \n                        #-- track --                  \n                        annotated_frame = frame\n                        x, y, w, h = box\n                        track = track_history[track_id]\n                        track.append((float(x), float(y)))  #-- x, y center point\n                        if len(track) > NUM_TRACK_THRESHOLD:  #-- retain NUM_TRACK_THRESHOLD tracks\n                            track.pop(0)\n\n                        #-- Draw the tracking lines --\n                        points = np.array(track, dtype=np.int32).reshape((-1, 1, 2))\n                        cv2.polylines(annotated_frame, [points], isClosed=False, color=(0, 0, 255), thickness=4)\n                        \n                        #-- Draw the bounding box --\n                        top_left = (int(x - w / 2), int(y - h / 2))\n                        bottom_right = (int(x + w / 2), int(y + h / 2))\n                        cv2.rectangle(annotated_frame, top_left, bottom_right, (255, 0, 0), 2)  #-- Blue bounding box\n\n                        #-- Put the ID text --\n                        text_position = (int(x - w / 2), int(y - h / 2) - 10)\n                        cv2.putText(annotated_frame, f'ID: {track_id}', text_position, cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n                else:\n                    annotated_frame = frame  #-- If no IDs, use original frame\n\n            else:\n                annotated_frame = frame  #-- If no boxes, use original frame\n\n            out.write(annotated_frame)\n\n        else:\n            #-- Break the loop if the end of the video is reached --\n            break\n\n    #-- Release the video capture object and close the display window --\n    cap.release()\n    out.release()   \n    #display.clear_output()\n    ","metadata":{"execution":{"iopub.status.busy":"2024-11-20T10:18:08.975733Z","iopub.execute_input":"2024-11-20T10:18:08.976045Z","iopub.status.idle":"2024-11-20T10:19:10.129438Z","shell.execute_reply.started":"2024-11-20T10:18:08.976018Z","shell.execute_reply":"2024-11-20T10:19:10.128455Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# zip_results = \"results\"\n# shutil.make_archive(zip_results, 'zip', results_dir)","metadata":{"execution":{"iopub.status.busy":"2024-11-20T05:58:39.424785Z","iopub.status.idle":"2024-11-20T05:58:39.425346Z","shell.execute_reply.started":"2024-11-20T05:58:39.425072Z","shell.execute_reply":"2024-11-20T05:58:39.425101Z"},"trusted":true},"outputs":[],"execution_count":null}]}